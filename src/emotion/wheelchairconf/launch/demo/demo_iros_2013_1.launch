<!-- Run a simulation using the voice and the head to control the wheelchair
using the destination inference and mode_selector.py.
It takes place in a given map, with known goals. To start moving autonomously
say "OK GO", to stopo say: "OK STOP"-->
<!-- Make sure that the microphone is connected and working :), put it in at half volume for better results-->
<!-- Make sure that you have internet to use google recognizer:) -->
<!-- Connect the wheelchair -->
<!-- Connect the kinect -->

<launch>
	<arg name="scenario" default="hall_inria" />
	<arg name="use_keyboard" default="0" /> <!-- if we want to use the keyboard to simulate the direction of the face instead using the kinect -->
	<arg name="mode_selector" default="1"/>

	### START STAGE ###
	<group ns="wheelchair">
		<include file="$(find wheelchair)/launch/bb_auto.launch" />
	</group>


	### If not using keyboard ###
	<group unless="$(arg use_keyboard)">
		### Start the kinect ###
		<arg name="camera" default="camera" />
		<include file="$(find openni_launch)/launch/openni.launch">
			<!-- "camera" should uniquely identify the device. All topics are pushed down
	       into the "camera" namespace, and it is prepended to tf frame ids. -->
			<arg name="camera" value="$(arg camera)"/>
			<!-- device_id can have the following formats:
	         "B00367707227042B": Use device with given serial number
	         "#1"              : Use first device found
	         "2@3"             : Use device on USB bus 2, address 3
		 	"2@0"             : Use first device found on USB bus 2
	    	-->
	  		<!--arg name="device_id" default="#1" /-->
		</include>
	</group>




	##### START MODE_SELECTOR ####
	<group ns="wheelchair">
		<param name="tf_prefix" value="wheelchair" />
		<node pkg="user_intentions" type="mode_selector.py" name="mode_selector" respawn="true" output="screen" />
    </group>



</launch>