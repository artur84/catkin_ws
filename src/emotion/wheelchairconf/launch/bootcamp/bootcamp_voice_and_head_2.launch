<!-- Run a simulation using the voice_and_head controller, to move say "ok wheelchair go", "ok wheelchair back", "ok wheelchair left"-->
<!-- Make sure that the microphone is connected and working :) -->
<!-- Make sure that you have internet to use google recognizer:) -->
<!-- Connect the kinect -->

<!-- prerequisites:
	roslaunch wheelchairconf start_bootcamp_voice_and_head_1.launch

I got many troubles when trying to launch openni.launch and kinect_face_pose_estimator.launch file together
	so normally you should first start openni.launch and then use this file -->



<launch>
	<arg name="planner" default="bootcamp" /><!--can also be: ui_planner, riskrrt -->
	<arg name="mode_selector" default="0"/><!--If we want to use (1) or not (0) the mode_selector node or pass the cmd_vel directly to the wheelchair -->
	<arg name="scenario" default="appart_nancy"/>
	<arg name="google" default= "1"/> <!-- set if using the google_recognizer.py node -->d

	####### Publish the map: It should be global so no namespace is defined #####
	<node pkg="map_server" type="map_server" name="map_server" output="log" args="$(find wheelchairconf)/world/$(arg scenario).yaml" respawn="true" >
		<param name="frame_id" value="/map" />
	</node>


	##### Start navigation #######
    <!-- In Real Demos -->
	<group ns="$(arg tf_prefix)">
		<param name="move_base/RiskRRTPlanner/plan_topic" value="$(arg tf_prefix)/plan"/>
	    <include file="$(find wheelchairconf)/launch/navigation/navigation_$(arg planner).launch" >
	    		<arg name="mode_selector" value="$(arg mode_selector)" />
	    		<arg name="planner" value="$(arg planner)"/>
	    		<arg name="tf_prefix" value="$(arg tf_prefix)"/>
	    </include>
		<node pkg="tf" type="static_transform_publisher" name="tf_map_to_wheelchair" args="0 0 0 0 0 0 /map $(arg tf_prefix)/map 40" />
	</group>


	### Start the face pose recognizer ###
	<!-- Calls the kinect face tracker, use this file to quickly access this
	in simulation /or demos while debugging -->

	<include file="$(find wheelchairconf)/launch/wheelchairint/kinect_face_pose_estimator.launch">
	    <arg name="tf_prefix" value="$(arg tf_prefix)"/>
	</include>


	### Start the recognizer ###
   	<!-- if we want to use google recognizer -->
    <group ns="$(arg tf_prefix)">
        <node name="google_recognizer" pkg="wheelchairint" type="wheelchair_recognizer.py" output="screen" respawn="True">
       	</node>
   	</group>

   	### Start voice and head command ###
   	<group ns="$(arg tf_prefix)">
        <node pkg="wheelchairint" type="voice_and_head.py" name="voice_and_head" respawn="true" output="screen">
	        <remap from="voice_and_head_vel" to="cmd_vel"/><!-- input for the wheelchair -->
        </node>
	</group>


</launch>
