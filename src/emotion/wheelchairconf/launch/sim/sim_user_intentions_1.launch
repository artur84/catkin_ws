<!-- Run a simulation using the user intentions estimation module with head_and_voice node-->
<!-- Make sure that the microphone is connected and working :) -->
<!-- Make sure that you have internet to use google recognizer:) -->
<!-- Connect the kinect -->


<!-- I got many troubles when trying to launch openni.launch and kinect_face_pose_estimator.launch file together
	so normally you should first start openni.launch and then use this file -->



<launch>
	<arg name="camera" default="camera" />
	#Start the kinect
	<include file="$(find openni_launch)/launch/openni.launch">
		<!-- "camera" should uniquely identify the device. All topics are pushed down
       into the "camera" namespace, and it is prepended to tf frame ids. -->
		<arg name="camera" value="$(arg camera)"/>
		<!-- device_id can have the following formats:
         "B00367707227042B": Use device with given serial number
         "#1"              : Use first device found
         "2@3"             : Use device on USB bus 2, address 3
	 	"2@0"             : Use first device found on USB bus 2
    	-->
  		<!--arg name="device_id" default="#1" /-->
	</include>

	### Start the recognizer ###
   	<!-- if we want to use google recognizer -->
    <group if="$(arg sim)" ns="robot_0">
        <node name="google_recognizer" pkg="wheelchairint" type="wheelchair_recognizer.py" output="screen" respawn="True">
       	</node>
   	</group>

	##### START RVIZ   #####
	<node pkg="rviz" type="rviz" name="rviz" args="-d $(find wheelchairconf)/config/rviz/sim_voice_and_head_cmd.rviz"/>


</launch>
